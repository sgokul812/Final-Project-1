{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "605e55cb-00ce-4c2d-bda4-353db5f640d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended products for customer CUST48d7aacb-aef0-4e57-99f0-a3818e72de23:\n",
      "- PRODee7f8e95-ea19-4460-b234-d061745e61d1\n",
      "- PRODfb1891de-e816-4cb8-a919-e22adee47529\n",
      "- PRODb8ecd3d0-67d5-405d-b1f7-6b63f5f134b7\n",
      "- PROD9be2925c-a49e-4055-a295-303b855d07e7\n",
      "- PROD02ea63b4-e10b-4388-807f-bd487449eee3\n",
      "Evaluation Metrics:\n",
      "- Precision: 0.99998\n",
      "- Recall: 0.99998\n",
      "- Mean Average Precision (MAP): 0.00002\n",
      "- Normalized Discounted Cumulative Gain (NDCG): 0.34649\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from faker import Faker\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import precision_score, recall_score, average_precision_score, ndcg_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Initializing faker to generate fake data\n",
    "fake = Faker()\n",
    "\n",
    "# Generating a larger dataset\n",
    "data = []\n",
    "for _ in range(20000):  # Generating 20,000 rows of data\n",
    "    customer_id = \"CUST\" + fake.uuid4()\n",
    "    product_id = \"PROD\" + fake.uuid4()\n",
    "    interaction_type = random.choice(['purchased', 'viewed', 'clicked'])\n",
    "    interaction_date = fake.date_time_this_year()\n",
    "    data.append([customer_id, product_id, interaction_type, interaction_date])\n",
    "\n",
    "# Creating a DataFrame\n",
    "df = pd.DataFrame(data, columns=['customer_id', 'product_id', 'interaction_type', 'interaction_date'])\n",
    "\n",
    "# Converting interaction type to numerical values\n",
    "interaction_map = {'purchased': 3, 'viewed': 2, 'clicked': 1}\n",
    "df['interaction_value'] = df['interaction_type'].map(interaction_map)\n",
    "\n",
    "# Creating additional features for content-based filtering\n",
    "df['interaction_day'] = df['interaction_date'].dt.day\n",
    "df['interaction_month'] = df['interaction_date'].dt.month\n",
    "df['interaction_year'] = df['interaction_date'].dt.year\n",
    "\n",
    "# Normalizing additional features\n",
    "scaler = StandardScaler()\n",
    "df[['interaction_day', 'interaction_month', 'interaction_year']] = scaler.fit_transform(df[['interaction_day', 'interaction_month', 'interaction_year']])\n",
    "\n",
    "# Creating a user-item matrix\n",
    "user_item_matrix = df.pivot_table(index='customer_id', columns='product_id', values='interaction_value', fill_value=0)\n",
    "\n",
    "# Converting to sparse matrix\n",
    "user_item_sparse = csr_matrix(user_item_matrix.values)\n",
    "\n",
    "# Splitting data into training and testing sets\n",
    "train_data, test_data = train_test_split(user_item_matrix, test_size=0.2, random_state=42)\n",
    "\n",
    "# Implementing the recommendation algorithm using Collaborative Filtering and Matrix Factorization\n",
    "svd = TruncatedSVD(n_components=50, random_state=42)  # Increasing n_components for better performance\n",
    "train_svd = svd.fit_transform(train_data)\n",
    "\n",
    "# Generating collaborative filtering predictions\n",
    "cf_pred_matrix = np.dot(train_svd, svd.components_)\n",
    "cf_pred_matrix = cf_pred_matrix[:train_data.shape[0], :user_item_matrix.shape[1]]\n",
    "\n",
    "# Generating content-based filtering predictions using cosine similarity\n",
    "content_sim_matrix = cosine_similarity(train_data)\n",
    "content_pred_matrix = np.dot(content_sim_matrix, train_data)\n",
    "\n",
    "# Adjusting weights for the hybrid approach\n",
    "alpha = 0.7  # Weight for collaborative filtering\n",
    "beta = 0.3  # Weight for content-based filtering\n",
    "combined_pred_matrix = alpha * cf_pred_matrix + beta * content_pred_matrix\n",
    "\n",
    "# Converting back to DataFrame\n",
    "pred_df = pd.DataFrame(combined_pred_matrix, index=user_item_matrix.index[:train_data.shape[0]], columns=user_item_matrix.columns)\n",
    "\n",
    "# Function for recommending products\n",
    "def recommend_products(customer_id, num_recommendations=5):\n",
    "    if customer_id in pred_df.index:\n",
    "        customer_predictions = pred_df.loc[customer_id]\n",
    "        recommended_products = customer_predictions.sort_values(ascending=False).head(num_recommendations).index.tolist()\n",
    "        return recommended_products\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "# Example recommendation for a customer with proper formatting\n",
    "example_customer_id = df['customer_id'].iloc[0]\n",
    "recommended_products = recommend_products(example_customer_id)\n",
    "print(f\"Recommended products for customer {example_customer_id}:\")\n",
    "for prod in recommended_products:\n",
    "    print(f\"- {prod}\")\n",
    "\n",
    "# Function to evaluate the model\n",
    "def evaluate_model(test_data, pred_matrix, threshold=2):\n",
    "    # Binarizing the interaction values based on the threshold\n",
    "    test_data_binary = (test_data > threshold).astype(int)\n",
    "    pred_data_binary = (pred_matrix > threshold).astype(int)\n",
    "\n",
    "    # Flattening the matrices for evaluation\n",
    "    test_data_flat = test_data_binary.values.flatten()\n",
    "    pred_data_flat = pred_data_binary.flatten()\n",
    "\n",
    "    # Calculating Precision and Recall\n",
    "    precision = precision_score(test_data_flat, pred_data_flat, average='micro')\n",
    "    recall = recall_score(test_data_flat, pred_data_flat, average='micro')\n",
    "    \n",
    "    # Calculating Mean Average Precision (MAP)\n",
    "    map_score = average_precision_score(test_data_flat, pred_data_flat)\n",
    "    \n",
    "    # Calculating Normalized Discounted Cumulative Gain (NDCG)\n",
    "    ndcg = ndcg_score([test_data_flat], [pred_data_flat])\n",
    "    \n",
    "    return precision, recall, map_score, ndcg\n",
    "\n",
    "# Generating test predictions\n",
    "test_svd = svd.transform(test_data)\n",
    "test_cf_pred_matrix = np.dot(test_svd, svd.components_)\n",
    "test_cf_pred_matrix = test_cf_pred_matrix[:, :user_item_matrix.shape[1]]\n",
    "\n",
    "# Generating content-based filtering predictions for the test set using cosine similarity\n",
    "test_content_sim_matrix = cosine_similarity(test_data)\n",
    "test_content_pred_matrix = np.dot(test_content_sim_matrix, test_data)\n",
    "\n",
    "# Combining collaborative and content-based predictions for the test set\n",
    "test_combined_pred_matrix = alpha * test_cf_pred_matrix + beta * test_content_pred_matrix\n",
    "\n",
    "# Evaluating the model\n",
    "precision, recall, map_score, ndcg = evaluate_model(test_data, test_combined_pred_matrix)\n",
    "\n",
    "# Improved metrics display\n",
    "print(f\"Evaluation Metrics:\\n\"\n",
    "      f\"- Precision: {precision:.5f}\\n\"\n",
    "      f\"- Recall: {recall:.5f}\\n\"\n",
    "      f\"- Mean Average Precision (MAP): {map_score:.5f}\\n\"\n",
    "      f\"- Normalized Discounted Cumulative Gain (NDCG): {ndcg:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d7c4ac-881f-4ac6-be72-71d435c17519",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
